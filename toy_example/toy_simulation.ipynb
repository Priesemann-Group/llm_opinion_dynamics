{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78211d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import toy_setup as setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9105a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your OpenAI API key here ###\n",
    "\n",
    "openai_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd113cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulation framework for climate change topic for normal and logically negated (reversed) framing ###\n",
    "\n",
    "def init_dataframe():\n",
    "    df = pd.DataFrame({\n",
    "        \"ID\": pd.Series(dtype='int'), \"TEXT_TYPE\": pd.Series(dtype='str'), \"INIT_OP_A\": pd.Series(dtype='int'), \n",
    "        \"TEXT_A\": pd.Series(dtype='str'), \"PPL_A\": pd.Series(dtype='float'), \"PROBE_INT_A\": pd.Series(dtype='float'), \n",
    "        \"H_INT_A\": pd.Series(dtype='float'), \"PROBE_EXT_A\": pd.Series(dtype='float'), \"H_EXT_A\": pd.Series(dtype='float'),\n",
    "        \"INIT_OP_B\": pd.Series(dtype='int'), \"TEXT_B\": pd.Series(dtype='str'), \"PPL_B\": pd.Series(dtype='float'),\n",
    "        \"PROBE_INT_B\": pd.Series(dtype='float'), \"H_INT_B\": pd.Series(dtype='float'), \"PROBE_EXT_B\": pd.Series(dtype='float'), \n",
    "        \"H_EXT_B\": pd.Series(dtype='float')\n",
    "    })\n",
    "    return df\n",
    "\n",
    "data_path = 'toy_runs/'\n",
    "\n",
    "def run_discussion(args):\n",
    "    topic = 'climate_change' \n",
    "    \n",
    "    gpt = setup.ChatGPT(model_id='gpt-4o-mini', api_key=openai_key)\n",
    "    gpt.config_generation()\n",
    "\n",
    "    alice = setup.ChatGPT(model_id='gpt-4o-mini', api_key=openai_key)\n",
    "    alice.generation_config = gpt.generation_config\n",
    "\n",
    "    bob = setup.ChatGPT(model_id='gpt-4o-mini', api_key=openai_key)\n",
    "    bob.generation_config = gpt.generation_config\n",
    "\n",
    "    tof_gen = setup.ChatGPT(model_id='gpt-4o-mini', api_key=openai_key)\n",
    "    tof_gen.generation_config = gpt.generation_config\n",
    "\n",
    "    disc_id, op_A, op_B = args\n",
    "    \n",
    "    D = 5\n",
    "    all_R = np.zeros((1, 1, D+1, 2, 2, 3))\n",
    "    all_probs = np.zeros((1, 1, D+1, 2, 2, 5, 2))\n",
    "\n",
    "    df = init_dataframe()\n",
    "    count = 0\n",
    "    pc = 0\n",
    "    n = 0\n",
    "\n",
    "    df.loc[count, 'ID'] = disc_id\n",
    "    df.loc[count, 'TEXT_TYPE'] = 'tot'\n",
    "    df.loc[count, 'INIT_OP_A'] = op_A\n",
    "    df.loc[count, 'INIT_OP_B'] = op_B\n",
    "\n",
    "    ### Generate train of thoughts \n",
    "    tot_A, out_tot_A, df.loc[count, 'PPL_A'] = tof_gen.generate_tof(opinion_level=op_A, subject_name=topic)\n",
    "    df.loc[count, 'TEXT_A'] = tot_A\n",
    "    tot_B, out_tot_B, df.loc[count, 'PPL_B'] = tof_gen.generate_tof(opinion_level=op_B, subject_name=topic)\n",
    "    df.loc[count, 'TEXT_B'] = tot_B\n",
    "\n",
    "    ### Initialize the LLM agents with the train of thoughts in the right format\n",
    "    alice.initiate_agent(subject_name=topic, tof=tot_A, role='system')\n",
    "    bob.initiate_agent(subject_name=topic, tof=tot_B, role='system')\n",
    "\n",
    "    ### probe internal opinion after initialization\n",
    "    all_R[pc, n, 0, 0, 0], all_probs[pc, n, 0, 0, 0] = alice.probe_internal(topic)\n",
    "    all_R[pc, n, 0, 1, 0], all_probs[pc, n, 0, 1, 0] = bob.probe_internal(topic)\n",
    "\n",
    "    df.loc[count, 'PROBE_INT_A'] = all_R[pc, n, 0, 0, 0][0]\n",
    "    df.loc[count, 'H_INT_A'] = all_R[pc, n, 0, 0, 0][2]\n",
    "    df.loc[count, 'PROBE_INT_B'] = all_R[pc, n, 0, 1, 0][0]\n",
    "    df.loc[count, 'H_INT_B'] = all_R[pc, n, 0, 1, 0][2]\n",
    "\n",
    "\n",
    "    #### set dca_type to get the right prompts at the start\n",
    "    dca_a = 'init_start'\n",
    "    dca_b = 'init_response'\n",
    "    response_B = \"\"\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    ### Loop over all discussion rounds\n",
    "    for d in range(D):\n",
    "\n",
    "        ### Save parameters\n",
    "        df.loc[count, 'ID'] = disc_id\n",
    "        df.loc[count, 'TEXT_TYPE'] = 'disc'\n",
    "        df.loc[count, 'INIT_OP_A'] = op_A\n",
    "        df.loc[count, 'INIT_OP_B'] = op_B\n",
    "\n",
    "        response_A, output_A, df.loc[count, 'PPL_A'] = alice.infer(content = setup.discussion_prompt(topic, dca_a, response_B))\n",
    "        df.loc[count, 'TEXT_A'] = response_A\n",
    "\n",
    "        response_B, output_B, df.loc[count, 'PPL_B'] = bob.infer(content = setup.discussion_prompt(topic, dca_b, response_A))\n",
    "        df.loc[count, 'TEXT_B'] = response_B\n",
    "\n",
    "        # probe opinion alice\n",
    "        prompt = setup.discussion_prompt(topic, 'reply_probe', response_B)\n",
    "        res, out, ppl = alice.infer(prompt, append=False)\n",
    "        all_R[pc, n, d+1, 0, 0], all_probs[pc, n, d+1, 0, 0] = alice.expected_opinion(out)\n",
    "\n",
    "        df.loc[count, 'PROBE_INT_A'] = all_R[pc, n, d+1, 0, 0][0]\n",
    "        df.loc[count, 'H_INT_A'] = all_R[pc, n, d+1, 0, 0][2]\n",
    "\n",
    "        # probe opinion bob\n",
    "        all_R[pc, n, d+1, 1, 0], all_probs[pc, n, d+1, 1, 0] = bob.probe_internal(topic)\n",
    "        df.loc[count, 'PROBE_INT_B'] = all_R[pc, n, d+1, 1, 0][0]\n",
    "        df.loc[count, 'H_INT_B'] = all_R[pc, n, d+1, 1, 0][2]\n",
    "\n",
    "        ### change dca_type for the rest of the discussion\n",
    "        dca_a = 'reply'\n",
    "        dca_b = 'reply'\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        np.save(data_path + 'normal/' + f'{disc_id}' + '_all_R', all_R)\n",
    "        np.save(data_path + 'normal/' + f'{disc_id}' + '_all_probs', all_probs)\n",
    "        df.to_csv(data_path + 'normal/' + f'{disc_id}' + '_messages.csv', index=False)    \n",
    "\n",
    "    return disc_id\n",
    "\n",
    "\n",
    "def run_discussion_rev(args):\n",
    "    topic = 'climate_change_rev'\n",
    "    tot_topic = 'climate_change'\n",
    "    tot_mapping = {1:5, 2:4, 3:3, 4:2, 5:1}\n",
    "    \n",
    "    gpt = setup.ChatGPT(model_id='gpt-4o-mini', api_key=openai_key)\n",
    "    gpt.config_generation()\n",
    "\n",
    "    alice = setup.ChatGPT(model_id='gpt-4o-mini', api_key=openai_key)\n",
    "    alice.generation_config = gpt.generation_config\n",
    "\n",
    "    bob = setup.ChatGPT(model_id='gpt-4o-mini', api_key=openai_key)\n",
    "    bob.generation_config = gpt.generation_config\n",
    "\n",
    "    tof_gen = setup.ChatGPT(model_id='gpt-4o-mini', api_key=openai_key)\n",
    "    tof_gen.generation_config = gpt.generation_config\n",
    "\n",
    "    disc_id, op_A, op_B = args\n",
    "    \n",
    "    D = 5\n",
    "    all_R = np.zeros((1, 1, D+1, 2, 2, 3))\n",
    "    all_probs = np.zeros((1, 1, D+1, 2, 2, 5, 2))\n",
    "\n",
    "    df = init_dataframe()\n",
    "    count = 0\n",
    "    pc = 0\n",
    "    n = 0\n",
    "\n",
    "    df.loc[count, 'ID'] = disc_id\n",
    "    df.loc[count, 'TEXT_TYPE'] = 'tot'\n",
    "    df.loc[count, 'INIT_OP_A'] = op_A\n",
    "    df.loc[count, 'INIT_OP_B'] = op_B\n",
    "\n",
    "    ### Generate train of thoughts \n",
    "    tot_A, out_tot_A, df.loc[count, 'PPL_A'] = tof_gen.generate_tof(opinion_level=tot_mapping[op_A], subject_name=tot_topic)\n",
    "    df.loc[count, 'TEXT_A'] = tot_A\n",
    "    tot_B, out_tot_B, df.loc[count, 'PPL_B'] = tof_gen.generate_tof(opinion_level=tot_mapping[op_B], subject_name=tot_topic)\n",
    "    df.loc[count, 'TEXT_B'] = tot_B\n",
    "\n",
    "    ### Initialize the LLM agents with the train of thoughts in the right format\n",
    "    alice.initiate_agent(subject_name=topic, tof=tot_A, role='system')\n",
    "    bob.initiate_agent(subject_name=topic, tof=tot_B, role='system')\n",
    "\n",
    "    ### probe internal opinion after initialization\n",
    "    all_R[pc, n, 0, 0, 0], all_probs[pc, n, 0, 0, 0] = alice.probe_internal(topic)\n",
    "    all_R[pc, n, 0, 1, 0], all_probs[pc, n, 0, 1, 0] = bob.probe_internal(topic)\n",
    "\n",
    "    df.loc[count, 'PROBE_INT_A'] = all_R[pc, n, 0, 0, 0][0]\n",
    "    df.loc[count, 'H_INT_A'] = all_R[pc, n, 0, 0, 0][2]\n",
    "    df.loc[count, 'PROBE_INT_B'] = all_R[pc, n, 0, 1, 0][0]\n",
    "    df.loc[count, 'H_INT_B'] = all_R[pc, n, 0, 1, 0][2]\n",
    "\n",
    "\n",
    "    #### set dca_type to get the right prompts at the start\n",
    "    dca_a = 'init_start'\n",
    "    dca_b = 'init_response'\n",
    "    response_B = \"\"\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    ### Loop over all discussion rounds\n",
    "    for d in range(D):\n",
    "\n",
    "        ### Save parameters\n",
    "        df.loc[count, 'ID'] = disc_id\n",
    "        df.loc[count, 'TEXT_TYPE'] = 'disc'\n",
    "        df.loc[count, 'INIT_OP_A'] = op_A\n",
    "        df.loc[count, 'INIT_OP_B'] = op_B\n",
    "\n",
    "        response_A, output_A, df.loc[count, 'PPL_A'] = alice.infer(content = setup.discussion_prompt(topic, dca_a, response_B))\n",
    "        df.loc[count, 'TEXT_A'] = response_A\n",
    "\n",
    "        response_B, output_B, df.loc[count, 'PPL_B'] = bob.infer(content = setup.discussion_prompt(topic, dca_b, response_A))\n",
    "        df.loc[count, 'TEXT_B'] = response_B\n",
    "\n",
    "        # probe opinion alice\n",
    "        prompt = setup.discussion_prompt(topic, 'reply_probe', response_B)\n",
    "        res, out, ppl = alice.infer(prompt, append=False)\n",
    "        all_R[pc, n, d+1, 0, 0], all_probs[pc, n, d+1, 0, 0] = alice.expected_opinion(out)\n",
    "\n",
    "        df.loc[count, 'PROBE_INT_A'] = all_R[pc, n, d+1, 0, 0][0]\n",
    "        df.loc[count, 'H_INT_A'] = all_R[pc, n, d+1, 0, 0][2]\n",
    "\n",
    "        # probe opinion bob\n",
    "        all_R[pc, n, d+1, 1, 0], all_probs[pc, n, d+1, 1, 0] = bob.probe_internal(topic)\n",
    "        df.loc[count, 'PROBE_INT_B'] = all_R[pc, n, d+1, 1, 0][0]\n",
    "        df.loc[count, 'H_INT_B'] = all_R[pc, n, d+1, 1, 0][2]\n",
    "\n",
    "        ### change dca_type for the rest of the discussion\n",
    "        dca_a = 'reply'\n",
    "        dca_b = 'reply'\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        np.save(data_path + 'reversed/' + f'{disc_id}' + '_all_R', all_R)\n",
    "        np.save(data_path + 'reversed/' + f'{disc_id}' + '_all_probs', all_probs)\n",
    "        df.to_csv(data_path + 'reversed/' + f'{disc_id}' + '_messages.csv', index=False)\n",
    "\n",
    "    return disc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run discussions with parallel processing API calls, this will take ~25 min for each framing ###\n",
    "args = []\n",
    "disc_id = 0\n",
    "\n",
    "for a in range(1, 6):\n",
    "    for b in range(1, 6):\n",
    "        for n in range(25):\n",
    "            args.append((disc_id, a, b))\n",
    "            disc_id += 1\n",
    "\n",
    "pool = ThreadPool()\n",
    "\n",
    "\n",
    "### Normal framing ###\n",
    "\n",
    "for idx in pool.imap_unordered(run_discussion, args):\n",
    "    print(f'Finished discussion {idx}', end='\\r')\n",
    "\n",
    "    \n",
    "### Reversed framing ###\n",
    "\n",
    "for idx in pool.imap_unordered(run_discussion_rev, args):\n",
    "    print(f'Finished discussion {idx}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68faffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all runs and prepare data for analysis\n",
    "\n",
    "df1 = pd.read_csv('toy_runs/normal/' + str(0) + '_messages.csv')\n",
    "\n",
    "for i in range(1, 625):\n",
    "    df2 = pd.read_csv('toy_runs/normal/' + str(i) + '_messages.csv')\n",
    "    df1 = pd.concat([df1, df2])\n",
    "\n",
    "df3 = pd.read_csv('toy_runs/reversed/' + str(0) + '_messages.csv')\n",
    "\n",
    "for i in range(1, 625):\n",
    "    df4 = pd.read_csv('toy_runs/reversed/' + str(i) + '_messages.csv')\n",
    "    df3 = pd.concat([df3, df4])\n",
    "\n",
    "\n",
    "# reset indices\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df1.to_csv('toy_runs/messages_normal.csv', index=False)\n",
    "df3.to_csv('toy_runs/messages_reversed.csv', index=False)\n",
    "\n",
    "# Process data to be ready for Bayesian inference\n",
    "\n",
    "paths = ['toy_runs/messages_normal.csv', 'toy_runs/messages_reversed.csv']\n",
    "\n",
    "dfs = [pd.read_csv(path) for path in paths]\n",
    "\n",
    "delta = [1, -1] # Add indicator for framing\n",
    "for i, df in enumerate(dfs):\n",
    "    df[\"d\"] = delta[i%2]\n",
    "\n",
    "\n",
    "# add new column for timstep\n",
    "for df in dfs:\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, 'timestamp'] = i%6\n",
    "\n",
    "# remove invalid probings\n",
    "for df in dfs:\n",
    "    idx_old = []\n",
    "\n",
    "    for i, (a, b) in enumerate(zip(df.loc[:, 'PROBE_INT_A'], df.loc[:, 'PROBE_INT_B'])):\n",
    "        if a < 1 or b < 1:\n",
    "            # save indeces to be removed\n",
    "            idx = df[df.loc[:, 'ID'] == df.loc[i, 'ID']].index\n",
    "            idx_old = np.concatenate([idx_old, idx])\n",
    "\n",
    "    df.drop(idx_old, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"Invalid probing:\", 100-len(df)/3750*100, \"%\")\n",
    "\n",
    "\n",
    "# convert opinion to -2 to 2\n",
    "\n",
    "for df in dfs:\n",
    "    df.loc[:, \"PROBE_INT_A\"] -= 3\n",
    "    df.loc[:, \"PROBE_INT_B\"] -= 3\n",
    "    df.loc[:, \"INIT_OP_A\"] -= 3\n",
    "    df.loc[:, \"INIT_OP_B\"] -= 3\n",
    "\n",
    "\n",
    "# concatentate all dfs\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Create new dataframe to hold processed data\n",
    "\n",
    "data = pd.DataFrame({\n",
    "        \"dx\": pd.Series(dtype='float'), \"x_i\": pd.Series('float'), \"x_j\": pd.Series('float'), \n",
    "        \"H_i\": pd.Series('float'), \"H_j\": pd.Series('float'),\n",
    "        \"delta\": pd.Series('int'), \"t\": pd.Series('int'), \"is_initiator\": pd.Series('int'),\n",
    "        \"init_x_i\": pd.Series('float'), \"init_x_j\": pd.Series('float')\n",
    "})\n",
    "\n",
    "\n",
    "i = 0\n",
    "q = 1\n",
    "\n",
    "for c in range(int(len(df))):\n",
    "\n",
    "    # first agent\n",
    "    data.loc[2*c, \"dx\"] = df.loc[i+1, \"PROBE_INT_A\"] - df.loc[i, \"PROBE_INT_A\"]\n",
    "    data.loc[2*c, \"x_i\"] = df.loc[i, \"PROBE_INT_A\"]\n",
    "    data.loc[2*c, \"x_j\"] = df.loc[i, \"PROBE_INT_B\"]\n",
    "\n",
    "    data.loc[2*c, \"delta\"] = df.loc[i, \"d\"]\n",
    "    data.loc[2*c, \"t\"] = df.loc[i, \"timestamp\"]\n",
    "    data.loc[2*c, \"is_initiator\"] = 1\n",
    "  \n",
    "    data.loc[2*c, \"H_i\"] = df.loc[i, \"H_INT_A\"]\n",
    "    data.loc[2*c, \"H_j\"] = df.loc[i, \"H_INT_B\"]\n",
    "    data.loc[2*c, \"init_x_i\"] = df.loc[i, \"INIT_OP_A\"]\n",
    "    data.loc[2*c, \"init_x_j\"] = df.loc[i, \"INIT_OP_B\"]\n",
    "\n",
    "    # second agent\n",
    "    data.loc[2*c + 1, \"dx\"] = df.loc[i+1, \"PROBE_INT_B\"] - df.loc[i, \"PROBE_INT_B\"]\n",
    "    data.loc[2*c + 1, \"x_i\"] = df.loc[i, \"PROBE_INT_B\"]\n",
    "    data.loc[2*c + 1, \"x_j\"] = df.loc[i, \"PROBE_INT_A\"]\n",
    "\n",
    "    data.loc[2*c + 1, \"delta\"] = df.loc[i, \"d\"]\n",
    "    data.loc[2*c + 1, \"t\"] = df.loc[i, \"timestamp\"]\n",
    "    data.loc[2*c + 1, \"is_initiator\"] = 0\n",
    "\n",
    "    data.loc[2*c + 1, \"H_i\"] = df.loc[i, \"H_INT_B\"]\n",
    "    data.loc[2*c + 1, \"H_j\"] = df.loc[i, \"H_INT_A\"]\n",
    "    data.loc[2*c + 1, \"init_x_i\"] = df.loc[i, \"INIT_OP_B\"]\n",
    "    data.loc[2*c + 1, \"init_x_j\"] = df.loc[i, \"INIT_OP_A\"]\n",
    "\n",
    "    if q == 5:\n",
    "        i += 2\n",
    "        q = 1\n",
    "    else:\n",
    "        i += 1\n",
    "        q += 1        \n",
    "    \n",
    "    if i==7500:\n",
    "        break\n",
    "\n",
    "    print(i, end='\\r')\n",
    "\n",
    "data.to_csv('your_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
